\subsection{Sistema a resolver}

Una vez obtenida la discretizaci\'on de nuestro sistema y la posici\'on de los puntos de calor en esta versi\'on como se mencion\'o con anterioridad, planteamos las ecuaciones que nos permitir\'an resolverlo. Consideremos $r$ al radio de alcance de los puntos de calor producidos por las sanguijuelas, $T_c$ su temperatura y $C_1$, ... , $C_k$ los $k$ puntos de calor, con $C_i \in R^2$, $C_i = (x_i, y_i)$.\\

Sabemos que si $ x = 0 \vee y = 0 \vee x = m \vee y = n \Rightarrow T(x,y) = -100$. \\
Tambi\'en que si $\exists \: C_j, \: C_j = (x_j, y_j) \: / \: \sqrt[2]{(x-x_j)^2 + (y-y_j)^2} \le r \Rightarrow T(x,y) = T_c$. \\
Luego, nos falta definir $T(x,y)$ para todos los $(x,y)$ que no son alcanzados por estas condiciones.\\
Ahora plantearemos $n \times m$ ecuaciones con $n \times m$ inc\'ognitas, que ser\'an el sistema que luego resolveremos. Las inc\'ognitas ser\'an los $T(x,y)$, y sus constantes correspondientes $a_{ij}$.\\

Sea $(\alpha,\beta)$ un punto de la grilla:
\begin{enumerate}
 \item Si es parte del borde, entonces su ecuaci\'on ser\'a $a_{ij}$ = -100 por la temperatura por defecto que toma esta secci\'on.
 \item Si es un punto donde existe una sanguijuela, este toma el valor de la constante de temperatura otorgado por esta. $a_{ij}$ = k donde K representa la temperatura pasada como input para dicha sanguijuela.
 \item Si en cambio es un punto del interior del plano que no se corresponde con la posici\'on de ninguna sanguijuela, su ecuaci\'on ser\'a, como mencionamos con anterioridad, 
\begin{equation}
t_{ij} \ =\ \frac{ t_{i-1,j} + t_{i+1,j} + t_{i,j-1} + t_{i,j+1}}{4}.
\end{equation}
 Teniendo en cuenta esta informaci\'on, podremos despejar la inc\'ognita para introducir dicha ecuaci\'on dentro de nuestra matriz, de la siguiente manera:
\begin{equation}
0 \ =\ \ -4t_{ij} + t_{i-1,j} + t_{i+1,j} + t_{i,j-1} + t_{i,j+1}.
\end{equation}
logrando as\'i igualar la ecuaci\'on a cero.
\end{enumerate}

Con esta nocion espacial de que cada punto esta condicionado tan solo por sus 4 vecinos, ahora quisieramos encontrar una manera de almacenarla que sea conveniente y nos permita reducir el espacio en la matriz y el numero de operaciones a realizar. En el siguiente apartado, describiremos como ordenando las variables de manera inteligente, podrémos guardar las incognitas en una matriz banda.

\subsection{Matriz banda}

Por lo antedicho en los aparatados anteriores, sabemos que todo punto del parabrisas, puede ser calculado en base a sus cuatro vecinos. Esto es, cualquier punto puede ser calculado en base al punto que se encuentra arriba de el, al punto que se encuentra debajo, al punto a la izquierda y a la derecha del mismo. Si ahora en la matriz de resolución del sistema lograsemos mantener a esos 4 puntos cerca de la diagonal, podíamos utilizar una matriz banda.
\\
Ahora supongamos que tenemos un pequeño paravrisas de $3 \times 3$ que queremos resolver. Lo que descubrimos tras un arduo analisis es que si ordenabamos cada punto tal que que la primera ecuacion es para el punto $t_{1,1}$, la segunda para $t_{1,2}$, la tercera para $t_{1,3}$, la cuarta para $t_{2,1}$, etc. nos quedará algo asi:

\setcounter{MaxMatrixCols}{30}

\[\begin{pmatrix}
 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 1 & 0 & 0 & 0 & 1 & -4 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\
 0 & 0 & 1 & 0 & 1 & 0 & 1 & -4 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\
 0 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & -4 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\
...
\end{pmatrix}
\begin{pmatrix} \\ x_{0,0}\\ x_{0,1}\\ x_{0,2}\\ x_{0,3} \\ x_{0,4} \\ x_{1,0}\\ x_{1,1} \\ x_{1,2} \\ x_{1,3} \\ ... \\ \end{pmatrix}
=
\begin{pmatrix}  \\ -100\\ -100\\ -100\\ -100\\ -100\\ -100 \\ 0 \\ 0 \\ 0 \\ ... \\ \end{pmatrix}
\]

Donde todo $x_{i,j}$ con $i$ o $j$ igual a $0$ o $4$ es uno de los bordes del parabrisas.

Entonces la matriz resultante queda banda, con el tamaño de la banda igual a una columna del parabrisas mas las dos columnas que representarian los bordes (en este caso las columnas 0 y 4).

\subsection{Almacenamiento de la matriz banda}

Debido a que la representaci\'on matricial del problema que atacamos posee una estructura donde los elemenos no nulos se concentran en la diagonal, pudimos representar esta matriz de $(nxm)x(nxm)$ usando otra estructura de $nx(2m+1)$ elementos, sabiendo que m\'as all\'a de la banda diagonal de $pxq$ elementos, el resto de la matriz se completa con 0. Al hacer esto debemos adaptar los algoritmos a esta representacion aportandonos reducir tanto la cantidad de operaciones como el espacio que ocupa la misma.

Todo este comportamiento de la matriz banda fue encapsulado en una clase para que fuera mas facil de utilizar y transparente tanto para el usuario como para las clases que utilizan esta matriz para setear u obtener sus elementos, sin que este necesite conocer la manera exacta en la que se almacenan los valores internamente.

\subsection{Eliminacion gaussiana sobre matriz banda}

Como primer forma de resolver el sistema de ecuaciones, se implemento un Gauss sobre el que se realizaron algunas mejoras. La idea general del algoritmo se mantiene, en el paso $k$ diagonalizamos la columna $k$ de la matriz $A$ y todos los elementos en la columna k-esima a partir del $a_{k,k}$ seran $0$.
\\
La mejora que implementaremos ser\'a que, sabiendo que la matriz A es banda, a partir del elemento $a_{k+p,k}$ (donde $p$ es el limite de la banda) los siguientes seran $0$ y a partir del elemento $a_{k,k+p}$ tambien seran $0$.
\\
Luego para la columna $k$ no ser\'a necesario chequear hasta el final de la matriz, sino hasta el valor $k+p$.
\\
Lo mismo pasa con las filas. En el gauss estandar, luego de diagonalizar una columna, todos los elementos de esa fila deben ser actualizados. Pero al trabajar con una matriz banda, solo es necesario actualizar los $p$ valores que siguen a la diagonal, ya que tenemos asegurado que todos los valores siguientes seran $0$.
\\
Escrito de manera mas formal el algoritmo ser\'a el siguiente:

\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Gauss(matriz A, vector b)}
  \STATE{Para k=$1...n$}
  \STATE{\quad Tomo el elemento $a_{i,i}$ como pivote}
  \STATE{\quad Para $i = k+1,...k+p$}  
  \STATE{\quad \quad Para $j = k+1,...i+p$}
  \STATE{\quad \quad \quad $a_{i,j}  = a_{i,j} - a_{k,j} * (a_{i,k} / a_{k,k})$}
  \STATE{\quad \quad $a_{i,k} = 0$}
\end{algorithmic}
\end{algorithm}

De este algoritmo se desprende que este algoritmo tiene una mejor complejidad comparado con el Gauss estandard, siendo la misma de $O(n*p^2)$ siendo $n$ la cantidad de incognitas de nuestra matriz, y $p$ el tamaño de la banda.

Una vez concluido el proceso de eliminaci\'on gaussiana sobre la matriz banda, utilizamos backwards substitution para resolver las ecuaciones y conseguir as\'i el valor de cada uno de los puntos de calor del plano discretizado. Aqu\'i tambien es posible realizar una optimizacion, ya que en cada paso no es necesario ir hasta la ultima columna sin\'o hasta la columna donde sabemos que comienza la banda.

De esta manera habremos disminuimos la complejidad del backwards substitution de $O(n^2)$ a una menor de $O(n*p)$.

\subsection{Descomposicion LU}

La segunda forma que desarrollamos para resolver el problema es realizando una descomposicion LU a la matriz original. La ventaja de esto es que en caso de actualizar el vector $b$, el costo de volver a obtener los resultados se reduce en $O(n^2)$.

Dado que la descomposicion LU es muy similar al algoritmo de eliminacion gaussiana, este tambien puede ser optimizado para ser utilizado en una matriz banda. Las optimizaciones ser\'an similares a las de Gauss. Expresandolo de manera algoritmica, la descomposicion $LU$ optimizada ser\'a asi:

\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Gauss(matriz A, vector b)}
  \STATE{Inicializo una matriz $L$ con unos en la diagonal}
  \STATE{Para k=$1...n$}
  \STATE{\quad Tomo el elemento $a_{k,k}$ como pivote}
  \STATE{\quad Para $i = k+1,...k+p$}  
  \STATE{\quad \quad Para $w = k+1,...i+p$}
  \STATE{\quad \quad \quad $a_{i,j}  = a_{i,j} - a_{k,j} * (a_{i,k} / a_{k,k})$}
  \STATE{\quad \quad $a_{i,k} = 0$}
  \STATE{\quad \quad $L_{i,k} = a_{k,j} * (a_{i,k} / a_{k,k}$} )
\end{algorithmic}
\end{algorithm}

Luego en la matriz $A$ obtengo la matriz diagonal superior y en el L la diagonal inferior.

Puede verse que siendo el algoritmo muy similar al de eliminacion gaussiana, tambien posee la misma complejidad, esto es $O(n*p^2)$.

Este algoritmo sera utilizado mas adelante, cuando queramos invertir una matriz de manera barata para poder utilizar el algoritmo de Sherman Morrison.

\subsection{No hay tiempo que perder}

En esta parte del trabajo, buscaremos la posibilidad de disminuir la temperatura del punto critico a travez de la eliminacion de una sanguijuela. Dado que esto nos afecta el sistema de ecuaciones, en primera instancia, ser\'a necesario recalcular todo el sistema otra vez, lo que podria resultar muy costoso. Plantear otra vez las ecuaciones, ahora con esta variaci\'on, y volver a calcular las inc\'ognitas mediante el algoritmo de eliminaci\'on gaussiana tomar\'ia otra vez complejidad $O(n*p^2)$. Teniendo en cuenta que es necesario calcular el sistema completo y luego una vez eliminando cada una de las sanguijuelas pegadas al parabrisas para saber si existe la posibilidad de salvarlo, hace que esta opci\'on no sea la mas adecuada, sobre todo en aquellos problemas donde el parabrisas presenta una granularidad muy fina. Para esto, entonces, hacemos uso de una variaci\'on de la f\'ormula de Sherman–Morrison, evitando replantear todo el sistema desde cero, permitiendo reutilizar informacion de la ejecucion con todas las sanguijuelas y disminuyendo la complejidad a una mas adecuada.
\\
Comencemos definiendo de manera mas adecuada la implementacion mas simple.


\subsection{Implementacion Simple}

Como ya hemos adelantado, la idea detras de este algoritmo sera quitar una sanguijuela del sistema, aplicarle eliminacion Gaussiana a la matriz obtenida y utilizar backwads substitution para obtener la solucion deseada.

Mas formalizado en pseudocidigo:

\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Ultimo\_disparo()}
\STATE{ Para cada sanguijuela}
\STATE{ \quad La quito de la matriz $A$}
\STATE{ \quad Quito su b en $0$}
\STATE{ \quad Aplico $Gauss(A,b)$}
\STATE{ \quad Aplico $Backwards\_substitution(A,b)$}
\STATE{ \quad Restablezco los valores originales en la matriz A y paso a la siguiente}
\end{algorithmic}
\end{algorithm}

Luego, siendo la cantidad de sanguijuelas $w$, el tamaño de la matriz de resoluci\'on $n$ y el tamaño de la banda $p$. La complejidad de este algoritmo sera $O(w*((n*p^2) + (n*p)) )$ que es lo mismo que $O(w*(n*p^2))$

Ahora intentaremos mejorar esta complejidad a travez de la utlizacion de la formula de sherman morrison.

\subsection{Implementacion Sherman-Morrison}
Entonces, sean $A$, $A^*$ la matriz original del problema y la matriz modificada sin una de sus sanguijuelas respectivamente, queremos resolver $A^*$ = $A + uv^t$ donde $uv^t$ representa la matriz que introduce los cambios debido a la modificacion. Sea entonces
$(A^*)x = b$. Resolvemos las siguientes ecuaciones, $Ay = b$ y $Az = u$ y obtenemos la nueva $x$ a partir del siguiente c\'alculo:
\begin{equation}
x = y - \frac{z(v^ty)}{1+v^tz}.
\end{equation}
As\'i entonces, podemos obtener los resultados a partir de multiplicaciones vectoriales de complejidad lineal y evitamos volver a usar el algoritmo de eliminaci\'on gaussiana de complejidad c\'ubica.
\\
Formalizando el algoritmo:
\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Ultimo\_disparo\_Sherman\_Morrison()}
\STATE{ Realizo la descomposición LU de A}
\STATE{ Para cada sanguijuela}
\STATE{ \quad Modifico el valor de b en el lugar de la sanguijuela}
\STATE{ \quad Calculo $x$ de la manera vista arriba}
\STATE{ \quad Restablezco los valores originales en b y paso a la siguiente sangijuela}
\end{algorithmic}
\end{algorithm}

Este algoritmo tendrá la siguiente complejidad:
\begin{enumerate}
 \item Descomposición LU: $O(np^2)$
 \item Luego para cada sangijuela:
\begin{enumerate}
\item Modifico el valor de b en el lugar de la sanguijuela: $O(1)$
\item Calculo $x$ de la manera vista arriba: $O(n*p)$
\item Restablezco los valores originales en b: $O(1)$
\end{enumerate}  
\end{enumerate}
Luego esto da una complejidad de $O(np^2 + wnp)$
