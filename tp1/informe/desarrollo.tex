\subsection{Sistema a resolver}

Una vez obtenida la discretización de nuestro sistema y las posiciones donde se aplicará calor, planteamos las ecuaciones que nos permitirán resolverlo. Consideremos $r$ al radio de alcance de los puntos donde se aplica calor, $T_c$ su temperatura y $C_1$, ... , $C_k$ los $k$ puntos donde se aplica calor, con $C_i \in R^2$, $C_i = (x_i, y_i)$.
\\
Sabemos que si $ x = 0$ o $y = 0$ o $x = m$ o $y = n$ entonces la temperatura en ese punto será:
$$T(x,y) = -100$$
También que $\forall \: C_j, \: C_j = (x_j, y_j) \: / \: \sqrt[2]{(x-x_j)^2 + (y-y_j)^2} \le r$ la temperatura de ese punto será:
$$T(x,y) = T_c$$ 
Los puntos restantes, es decir todos los $(x,y)$ que no cumplen ninguna de estas condiciones tendrán que ser calculados con la ecuación planteada previamente en la introducción.
\\
En otras palabras lo que haremos será plantear $n \times m$ ecuaciones con $n \times m$ incógnitas. Cada una de estas ecuaciones representa un punto del parabrisas y cada uno de estos puntos vendrá definido de la siguiente manera:
\\
Sea $t_{ij}$ un punto de la grilla:
\begin{enumerate}
 \item Si es parte del borde ($ i = 0$ o $j = 0$ o $i = m$ o $i = n$) entonces su ecuación es $t_{ij} = -100$ por la temperatura por defecto que toma esta sección.
 \item Si es un punto donde existe una sanguijuela, sea $k$ la temperatura de la misma, entonces la ecuación será $t_{ij} = k$.
 \item Si en cambio es un punto del interior del plano que no se corresponde con la posición de ninguna sanguijuela, su ecuación será, como mencionamos con anterioridad, 
\begin{equation}
t_{ij} \ =\ \frac{ t_{i-1,j} + t_{i+1,j} + t_{i,j-1} + t_{i,j+1}}{4}.
\end{equation}
\end{enumerate}

Con esta noción espacial, asumiendo cada punto está condicionado tan solo por sus 4 vecinos, ahora se presenta el objetivo encontrar una manera de almacenar la matriz que sea conveniente y nos permita reducir el espacio en la matriz y el número de operaciones a realizar. En el siguiente apartado describiremos como ordenando las variables de manera inteligente, podremos guardar las incógnitas en una matriz banda.

\subsection{Matriz banda}

Por lo antedicho, sabemos que todo punto interno del parabrisas puede ser calculado en base a sus cuatro vecinos. Esto es, cualquier punto puede ser calculado en base al punto que se encuentra arriba de el, al punto que se encuentra debajo, al punto a la izquierda y a la derecha del mismo. Si logramos mantener en la matriz de resolución del sistema esos 4 puntos cercanos a la diagonal, podemos utilizar una matriz banda.
\\
Ahora supongamos que tenemos un pequeño sistema parabrisas de $3 \times 3$ que queremos resolver. Lo que descubrimos tras un arduo análisis es que si ordenamos cada punto tal que la primera ecuación es para el punto $t_{1,1}$, la segunda para $t_{1,2}$, la tercera para $t_{1,3}$, la cuarta para $t_{2,1}$, etc. se forma una matriz con el siguiente contenido:

\setcounter{MaxMatrixCols}{30}

\[\begin{pmatrix}
 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
 0 & 1 & 0 & 0 & 0 & 1 & -4 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\
 0 & 0 & 1 & 0 & 1 & 0 & 1 & -4 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\
 0 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & -4 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\
...
\end{pmatrix}
\begin{pmatrix} \\ x_{0,0}\\ x_{0,1}\\ x_{0,2}\\ x_{0,3} \\ x_{0,4} \\ x_{1,0}\\ x_{1,1} \\ x_{1,2} \\ x_{1,3} \\ ... \\ \end{pmatrix}
=
\begin{pmatrix}  \\ -100\\ -100\\ -100\\ -100\\ -100\\ -100 \\ 0 \\ 0 \\ 0 \\ ... \\ \end{pmatrix}
\]

Donde todo $x_{i,j}$ con $i$ o $j$ igual a $0$ ó $4$ es uno de los bordes del parabrisas.
\\
Entonces la matriz resultante queda banda, con el tamaño de la banda igual a una columna del parabrisas más las dos columnas que representan los bordes (en este caso las columnas $0$ y $4$).

\subsection{Almacenamiento de la matriz banda}

Debido a la representación matricial definida previamente donde logramos concentrar los elementos no nulos cerca en la diagonal, pudimos representar esta matriz de $(nxm)x(nxm)$ usando otra estructura de $nx(2m+1)$ elementos, sabiendo que más allá de la banda diagonal de $pxq$ elementos, el resto de la matriz se completa con $0$. Esto nos permitirá reducir tanto la complejidad espacial como la complejidad temporal de los algoritmos.
\\
El comportamiento de la matriz banda fue encapsulado en una clase para que fuera más fácil de utilizar y transparente tanto para el usuario como para las clases que utilizan esta matriz para setear u obtener sus elementos, sin que este necesite conocer la manera exacta en la que se almacenan los valores internamente.

\subsection{Eliminación gaussiana sobre matriz banda}

Como primer forma de resolver el sistema de ecuaciones, se implemento el algoritmo de eliminación gaussiana sobre el que se realizaron algunas mejoras. La idea general del algoritmo se mantiene. En el paso $k$ se realiza la diagonalización de la columna $k$ de la matriz $A$ y todos los elementos en la columna k-esima a partir del $a_{k,k}$ pasan a tener valor $0$.
\\
La mejora implementada consta en que, sabiendo que la matriz $A$ es banda, a partir del elemento $a_{k+p,k}$ (donde $p$ es el límite de la banda) los siguientes serán $0$ y a partir del elemento $a_{k,k+p}$ también serán $0$.
\\
Luego, para la columna $k$ no será necesario chequear hasta el final de la matriz, sino hasta el valor $k+p$.
\\
Lo mismo pasa con las filas. En el Gauss estándar, luego de diagonalizar una columna, todos los elementos de esa fila deben ser actualizados. Pero al trabajar con una matriz banda, solo es necesario actualizar los $p$ valores que siguen a la diagonal, ya que tenemos asegurado que todos los valores siguientes son $0$.
\\
Expresando de manera más formal, el algoritmo es el siguiente:

\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Gauss(matriz A, vector b)}
  \STATE{Para k=$1...n$}
  \STATE{\quad Tomo el elemento $a_{k,k}$ como pivote}
  \STATE{\quad Para $i = k+1,...k+p$}  
  \STATE{\quad \quad Para $j = k+1,...i+p$}
  \STATE{\quad \quad \quad $a_{i,j}  = a_{i,j} - a_{k,j} * (a_{i,k} / a_{k,k})$}
  \STATE{\quad \quad $a_{i,k} = 0$}
\end{algorithmic}
\end{algorithm}

Este algoritmo posee una mejor complejidad comparado con el Gauss estándar, siendo la misma de $O(n*p^2)$ con $n$ la cantidad de incógnitas de nuestra matriz, y $p$ el tamaño de la banda.
\\
Una vez concluido el proceso de eliminación gaussiana sobre la matriz banda, utilizamos backward substitution para resolver las ecuaciones y conseguir así el valor de cada uno de los puntos de calor del plano discretizado. Aquí también es posible realizar una optimización, ya que en cada paso no es necesario avanzar hasta la ultima columna sino hasta la columna donde sabemos que comienza la banda. De esta manera tambien es posible disminuir la complejidad del backward substitution de $O(n^2)$ a una menor de $O(n*p)$.
\\
Luego la resolución de este sistema tendrá una complejidad temporal de $O(n*p^2)$.

\subsection{Descomposición LU}

La segunda forma que desarrollamos para resolver el problema es realizando una descomposición LU sobre matriz original.

Dado que la descomposición LU es muy similar al algoritmo de eliminación gaussiana, este también puede ser optimizado para ser utilizado en una matriz banda. Las optimizaciones serán similares a las de la eliminación gaussiana. Expresándolo de manera algorítmica, la descomposición $LU$ optimizada será así:

\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Gauss(matriz A, vector b)}
  \STATE{Inicializo una matriz $L$ con unos en la diagonal}
  \STATE{Para k=$1...n$}
  \STATE{\quad Tomo el elemento $a_{k,k}$ como pivote}
  \STATE{\quad Para $i = k+1,...k+p$}  
  \STATE{\quad \quad Para $w = k+1,...i+p$}
  \STATE{\quad \quad \quad $a_{i,j}  = a_{i,j} - a_{k,j} * (a_{i,k} / a_{k,k})$}
  \STATE{\quad \quad $a_{i,k} = 0$}
  \STATE{\quad \quad $L_{i,k} = a_{k,j} * (a_{i,k} / a_{k,k}$} )
\end{algorithmic}
\end{algorithm}

Luego en la matriz $A$ obtengo la matriz diagonal superior y en el L la diagonal inferior que, como puede verse fácilmente, también son banda ya que los valores fuera de la banda nunca son modificados.
\\
Puede verse que siendo el algoritmo muy similar al de eliminación gaussiana, también posee la misma complejidad, esto es $O(n*p^2)$.
\\
Como ya habíamos dicho, el backward substitution para la matriz banda tiene una complejidad de $O(n*p)$ y de manera similar el foward substitution tiene la misma complejidad.
\\
Luego la complejidad de este método también será de $O(n*p^2)$. La ventaja de este método con respecto al anterior es que en caso de actualizar el vector $b$, el costo de volver a obtener los resultados se reduce en $O(n*p)$, ya que solo es necesario realizar un backward substitution y un foward substitution. Esta ventaja será muy útil más adelante, cuando queramos invertir una matriz de manera barata para poder utilizar el algoritmo de Sherman Morrison.

\subsection{No hay tiempo que perder}

En esta sección del trabajo, buscaremos la posibilidad de disminuir la temperatura del punto crítico a través de la eliminación de una sanguijuela. Dado que esto afecta el sistema de ecuaciones, en primera instancia será necesario recalcular el sistema completo otra vez, lo que podría resultar muy costoso.
\\
Plantear otra vez las ecuaciones, ahora con esta variación, y volver a calcular las incógnitas mediante el algoritmo de eliminación gaussiana tomaría otra vez complejidad $O(n*p^2)$. Teniendo en cuenta que es necesario calcular el sistema completo y luego una vez eliminando cada una de las sanguijuelas pegadas al parabrisas para saber si existe la posibilidad de salvarlo, hace que esta opción no sea la más adecuada, sobre todo en aquellos problemas donde el parabrisas presenta una gran cantidad de sanguijuelas.
\\
Para esto, entonces, hacemos uso de una variación de la fórmula de Sherman–Morrison, evitando replantear todo el sistema desde cero, permitiendo reutilizar información de la ejecución con todas las sanguijuelas y disminuyendo la complejidad a una más adecuada.
\\
Comencemos definiendo de manera más adecuada la implementación más simple.

\subsection{Implementación simple}

Como ya hemos adelantado, la idea detrás de este algoritmo es la de quitar una sanguijuela del sistema, aplicarle eliminación Gaussiana a la matriz obtenida y utilizar backward substitution para obtener la solución deseada.

Utilizando pseudocódigo, el procedimiento puede expresarse de la siguiente manera:

\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Ultimo\_disparo()}
\STATE{ Para cada sanguijuela}
\STATE{ \quad La quito de la matriz $A$}
\STATE{ \quad Quito su b en $0$}
\STATE{ \quad Aplico $Gauss(A,b)$}
\STATE{ \quad Aplico $Backwards\_substitution(A,b)$}
\STATE{ \quad Restablezco los valores originales en la matriz A y paso a la siguiente}
\end{algorithmic}
\end{algorithm}

Luego, siendo la cantidad de sanguijuelas $w$, el tamaño de la matriz de resolución $n$ y el tamaño de la banda $p$. La complejidad de este algoritmo será $O(w*((n*p^2) + (n*p)) )$ que es lo mismo que $O(w*(n*p^2))$

Ahora intentaremos mejorar esta complejidad a través de la utilización de la fórmula de Sherman-Morrison.

\subsection{Implementación Sherman-Morrison}

Sean $A$ la matriz original del problema y $A^*$ la matriz modificada sin una de sus sanguijuelas, queremos resolver $A^* = A + uv^t$ donde $uv^t$ representa la matriz que introduce los cambios debido a la modificación del sistema. Sea entonces
$(A^*)x = b$. Resolvemos las siguientes ecuaciones, $Ay = b$ y $Az = u$ y obtenemos la nueva $x$ a partir del siguiente cálculo:
\begin{equation}
x = y - \frac{z(v^ty)}{1+v^tz}.
\end{equation}
Así entonces, podemos obtener los resultados a partir de multiplicaciones vectoriales de complejidad lineal y evitamos volver a usar el algoritmo de eliminación gaussiana de complejidad cúbica.
\\

Formalizando el algoritmo:
\begin{algorithm}
\begin{algorithmic}[1]\parskip=1mm
\caption{void Ultimo\_disparo\_Sherman\_Morrison()}
\STATE{ Realizo la descomposición LU de A}
\STATE{ Para cada sanguijuela}
\STATE{ \quad Modifico el valor de b en el lugar de la sanguijuela}
\STATE{ \quad Calculo $x$ de la manera vista arriba}
\STATE{ \quad Restablezco los valores originales en b y paso a la siguiente sanguijuela}
\end{algorithmic}
\end{algorithm}

Este algoritmo tendrá la siguiente complejidad:
\begin{enumerate}
 \item Descomposición LU: $O(np^2)$
 \item Luego para cada sanguijuela:
\begin{enumerate}
\item Modifico el valor de b en el lugar de la sanguijuela: $O(1)$
\item Calculo $x$ utilizando el procedimiento mencionado anteriormente: $O(n*p)$
\item Restablezco los valores originales en b: $O(1)$
\end{enumerate}  
\end{enumerate}
Luego esto da una complejidad de $O(np^2 + wnp)$
